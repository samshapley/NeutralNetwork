-------------------------------------------------------------------------------- youtube.py --------------------------------------------------------------------------------  from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from ai import AI
from utils import load_prompts, parse_bullets
import openai
import pprint
import json
from transcribe import transcribe_video

class YouTubeSearch:
    def __init__(self, api_key):
        self.api_key = api_key
        self.youtube = build('youtube', 'v3', developerKey=api_key)
        self.prompts = load_prompts()

    def search_videos(self, query, max_results=2):
        try:
            search_response = self.youtube.search().list(
                q=query,
                part='id,snippet',
                maxResults=max_results
            ).execute()

            return search_response

        except HttpError as e:
            print(f'An error occurred: {e}')
            return None

    def get_video_comments(self, video_id, max_results=100):
        try:
            comments = []
            results = self.youtube.commentThreads().list(
                part='id,snippet',
                videoId=video_id,
                maxResults=max_results,
                textFormat='plainText',
                order = 'relevance'
            ).execute()

            for item in results['items']:
                comment = item['snippet']['topLevelComment']['snippet']
                comments.append({
                    'author': comment['authorDisplayName'],
                    'text': comment['textOriginal'],
                    'likes': comment['likeCount'],
                    'published_at': comment['publishedAt']
                })

            return comments

        except HttpError as e:
            print(f'An error occurred: {e}')
            return None

    def save_json_to_file(self, query, search_response, filename='search_response.json'):
        # Read existing JSON file
        with open(filename, 'r') as f_in:
            searches = []

        # Update JSON file with new search results
        with open(filename, 'w') as f_out:
            # Create new search object to append to existing list
            for item in search_response:

                video_id = item['id']['videoId']
                link = f'https://www.youtube.com/watch?v={video_id}'
                transcript = transcribe_video(link)

                search_object = {
                    "query": query,
                    "search_response": item,
                    "transcript": transcript
                }

                searches.append(search_object)
                json_data['searches'] = searches

                f_out.write(json_data)

    def search_loop(self, query):
        # Call the search_videos function once with user-specified input
        search_response = self.search_videos(query)["items"]

        self.save_json_to_file(query, search_response)

        # with open('search_response.json', 'r') as f:
        #     search_response = json.load(f)
        #     print(search_response)
        #     del search_response['searches'][0]['transcript']

        #     ai = AI(system=self.prompts["NEW_SEARCH"], openai_module=openai)
        #     prompt =  str(search_response)
        #     response, messages = ai.generate_response(prompt)
            
        #     # Parse response
        #     queries = parse_bullets(response)
            
        #     for query in queries:
        #         search_response = self.search_videos(query)
        #         self.save_json_to_file(query, search_response)

  -------------------------------------------------------------------------------- ai.py --------------------------------------------------------------------------------  class AI:
    def __init__(self, openai_module, system="",rate=150):
        self.system = system
        self.openai = openai_module
        self.messages = [{"role": "system", "content": system}]        
        
    def generate_response(self, prompt):
        self.messages.append({"role": "user", "content": prompt})
        
        response_json = self.openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=self.messages,
        )

        response_text = response_json["choices"][0]["message"]["content"]

        # Print the AI's response as it's being read out
        print(f"User: {prompt}\nAssistant: {response_text}\n")

        self.messages.append({"role": "assistant", "content": response_text})

        return response_text, self.messages  -------------------------------------------------------------------------------- utils.py --------------------------------------------------------------------------------  my_dict = {}

def load_prompts():
    with open('PROMPTS.txt', 'r') as f:
        for line in f:
            key, value = line.split(':')
            my_dict[key] = value

    return my_dict


def parse_bullets(bullets):
    # Given a string in the following format:
    # 1. "string 1"
    # 2. "string 2"
    # and so forth
    # Extract the strings and return them in a list
    
    strings = []
    for line in bullets.split('\n'):
        print(line)
        if line.strip():
            string = line.split('. ')[1]
            strings.append(string)

    return(strings)

# s = "1. Climate change Republican policy absurd\n"
# print(parse_bullets(s))
    
  -------------------------------------------------------------------------------- transcribe.py --------------------------------------------------------------------------------  from pytube import YouTube
import os
import whisper

def transcribe_video(link):
    model = whisper.load_model("tiny")

    yt = YouTube(link, use_oauth=True)

    print("Downloading Audio")
    audio = yt.streams.get_audio_only()
    audio_file = "temp.mp3"
    audio.download(filename=audio_file)

    print("Transcribing Video")
    result = model.transcribe(audio_file)
    
    return result["text"]
  -------------------------------------------------------------------------------- caption_image.py --------------------------------------------------------------------------------  from transformers import pipeline
from PIL import Image

def generate_caption(image_path):
    image_to_text = pipeline("image-to-text", model="nlpconnect/vit-gpt2-image-captioning")
    image = Image.open(image_path)
    caption = image_to_text(image)[0]["generated_text"]
    return caption  -------------------------------------------------------------------------------- prompt.py --------------------------------------------------------------------------------  ##Â A prompt.py file is a file creates a prompt for ChatGPT of the codebase to make it easy.

import os

def get_code_from_file(file_path):
    with open(file_path, 'r') as f:
        return f.read()

# Get the current directory
current_dir = os.getcwd()

# Get the list of files in the current directory
files = os.listdir(current_dir)

# Loop through the files and save the code to a text file, separated by dashes, and the file name
with open('code.txt', 'w') as f:
    for file in files:
        if file.endswith('.py'):
            code = get_code_from_file(file)
            f.write('-' * 80 + ' ' + file + ' ' + '-' * 80 + ' ' + ' ' + code + ' ' + ' ') #add a space at the end to make sure the next file starts on a new line  -------------------------------------------------------------------------------- main.py --------------------------------------------------------------------------------  from youtube import YouTubeSearch
import openai
from ai import AI

# Set up OpenAI API key
openai.api_key = "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"
youtube_api_key = 'AIzaSyBA43hoEQmwUEBynYWjQKsazVnRr6ybFb0'


query = input("Enter your search query: ")

youtube_search = YouTubeSearch(youtube_api_key)

# search_response = youtube_search.search_videos(query)
youtube_search.search_loop(query)

# if search_response:
#     youtube_search.save_json_to_file(search_response)
# Get the video ID from the search response

  